# Parameters for Project folder path.
# Used for the IMT only. In general, the project folder and the dataset
# can be specified as args of the command running the application
project_folder_path: /data/dc-models
dataset_path: /data/datasets 

# Parameters for the corpus loaders
corpus:
  # Fraction of documents to be taken from the corpora
  # Warning: do not use exponential notation (e.g. use 0.01 and not 1e-2)
  sampling_factor:
    SemanticScholar: 0.01
    SemanticScholar_emb: 0.01
    patstat: 0.015
    patstat_emb: 0.015 

# Parameter for the keyword-based document selector
keywords:
  # Selection method: 'embedding' or 'count'
  method: 'count'
  # Weight of the title. A word in the title is equivalent to wt repetitions
  # of the word in the description. (For method='count' only)
  wt: 1 
  # Maximum number of documents to be selected.
  n_max: 50000
  # Minimum score. Only docs scored strictly above s_min are selected
  s_min: 0.6
  # Name of the SBERT model. Available pretrained models can be found in 
  # https://www.sbert.net/docs/pretrained_models.html
  model_name: all-MiniLM-L6-v2

# Parameter for the zero-shot document selector
zeroshot:
  # Maximum number of documents to be selected.
  n_max: 4000
  # Minimum score. Only docs scored strictly above s_min are selected
  s_min: 0.6

# Parameter for the topic-based document selector
topics:
  # Maximum number of documents to be selected.
  n_max: 4000
  # Minimum score. Only docs scored strictly above s_min are selected
  s_min: 0.2

# Parameters for the classifier
classifier:
  # Type of transformer for ClassificationModel (e.g. roberta, mpnet), see
  # https://simpletransformers.ai/docs/classification-specifics/#supported-model-types
  model_type: mpnet
  # Name of the simpletransformer model (e.g. roberta_base, all-mpnet-base-v2)
  model_name: sentence-transformers/all-mpnet-base-v2
  # Maximum ratio negative vs positive samples in the training set
  max_imbalance: 3
  # Maximum number of documents in the training set.
  nmax: 400
  # If True, the embedding layer is fixed, and not fine-tuned.
  # During learning, only the layers aftert the encoder of the sentence
  # transformer are updated.
  freeze_encoder: True
  # Number of epochs
  epochs: 5
  # Only for GPU use
  batch_size: 8

# Parameters for the active learning (AL)
active_learning:
  # Number of docs to show each AL round
  n_docs: 8
  # Sample selection algorithm: 'random' or 'extremes'
  sampler: 'extremes'
  # Ratio of high-score samples. The rest will be low-score samples.
  # (Used for sampler='extremes' only)
  p_ratio: 0.8
  # (Approximate) probability of selecting the doc with the highest score in a
  # single sampling. This parameter is used to control the randomness of the
  # stochastic sampling: if top_prob=1, the samples with the highest score are 
  # are taken deterministically. top_prob=0 is equivalent to random sampling.
  # (Used for sampler='extremes' only)
  top_prob: 0.1

# Specify format for the log outputs
logformat:
  filename:    msgs.log
  datefmt:     '%m-%d %H:%M:%S'
  file_format: '%(asctime)s %(levelname)-8s %(message)s'
  file_level:  INFO
  cons_level:  DEBUG
  cons_format: '%(levelname)-8s %(message)s'